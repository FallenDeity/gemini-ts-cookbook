{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting with an Apollo 11 transcript\n",
    "\n",
    "This notebook provides a quick example of how to prompt Gemini using a text file. In this case, you'll use a 400 page transcript from [Apollo 11](https://www.nasa.gov/history/alsj/a11/a11trans.html).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install the Google GenAI SDK\n",
    "\n",
    "Install the Google GenAI SDK from [npm](https://www.npmjs.com/package/@google/genai). \n",
    "\n",
    "```bash\n",
    "$ npm install @google/genai\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your API key\n",
    "\n",
    "You can [create](https://aistudio.google.com/app/apikey) your API key using Google AI Studio with a single click.\n",
    "\n",
    "Remember to treat your API key like a password. Don't accidentally save it in a notebook or source file you later commit to GitHub. In this notebook we will be storing the API key in a `.env` file. You can also set it as an environment variable or use a secret manager. \n",
    "\n",
    "Here's how to set it up in a `.env` file:\n",
    "\n",
    "```bash\n",
    "$ touch .env\n",
    "$ echo \"GEMINI_API_KEY=<YOUR_API_KEY>\" >> .env\n",
    "```\n",
    "\n",
    ":::{.callout-tip}\n",
    "\n",
    "Another option is to set the API key as an environment variable. You can do this in your terminal with the following command:\n",
    "\n",
    "```bash\n",
    "$ export GEMINI_API_KEY=\"<YOUR_API_KEY>\"\n",
    "```\n",
    ":::\n",
    "\n",
    "### Load the API key\n",
    "\n",
    "To load the API key from the `.env` file, we will use the `dotenv` package. This package loads environment variables from a `.env` file into `process.env`. \n",
    "\n",
    "```bash\n",
    "$ npm install dotenv\n",
    "```\n",
    "\n",
    "Then, we can load the API key in our code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY is set in the environment variables\n"
     ]
    }
   ],
   "source": [
    "const dotenv = require(\"dotenv\") as typeof import(\"dotenv\");\n",
    "\n",
    "dotenv.config({\n",
    "  path: \"../.env\",\n",
    "});\n",
    "\n",
    "const GEMINI_API_KEY = process.env.GEMINI_API_KEY ?? \"\";\n",
    "if (!GEMINI_API_KEY) {\n",
    "  throw new Error(\"GEMINI_API_KEY is not set in the environment variables\");\n",
    "}\n",
    "console.log(\"GEMINI_API_KEY is set in the environment variables\");\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "In our particular case the `.env` is is one directory up from the notebook, hence we need to use `../` to go up one directory. If the `.env` file is in the same directory as the notebook, you can omit it altogether. \n",
    "\n",
    "```\n",
    "\u2502\n",
    "\u251c\u2500\u2500 .env\n",
    "\u2514\u2500\u2500 examples\n",
    "    \u2514\u2500\u2500 Apollo_11.ipynb\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SDK Client\n",
    "\n",
    "With the new SDK, now you only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "const google = require(\"@google/genai\") as typeof import(\"@google/genai\");\n",
    "\n",
    "const ai = new google.GoogleGenAI({ apiKey: GEMINI_API_KEY });\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model\n",
    "\n",
    "Now select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](quickstarts/Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "const tslab = require(\"tslab\") as typeof import(\"tslab\");\n",
    "\n",
    "const MODEL_ID = \"gemini-2.5-flash-preview-05-20\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Apollo 11 transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require(\"fs\") as typeof import(\"fs\");\n",
    "const path = require(\"path\") as typeof import(\"path\");\n",
    "\n",
    "const downloadFile = async (url: string, filePath: string) => {\n",
    "  const response = await fetch(url);\n",
    "  if (!response.ok) {\n",
    "    throw new Error(`Failed to download file: ${response.statusText}`);\n",
    "  }\n",
    "  fs.mkdirSync(path.dirname(filePath), { recursive: true });\n",
    "  const buffer = await response.blob();\n",
    "  const bufferData = Buffer.from(await buffer.arrayBuffer());\n",
    "  fs.writeFileSync(filePath, bufferData);\n",
    "};\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const TRANSCRIPT_URL = \"https://storage.googleapis.com/generativeai-downloads/data/a11.txt\";\n",
    "const transcriptFilePath = path.join(\"../assets\", \"a11.txt\");\n",
    "await downloadFile(TRANSCRIPT_URL, transcriptFilePath);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the file using the File API so its easier to pass it to the model later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { File, FileState } from \"@google/genai\";\n",
    "\n",
    "async function deferredFileUpload(filePath: string, config: { displayName: string }): Promise<File> {\n",
    "  const file = await ai.files.upload({\n",
    "    file: filePath,\n",
    "    config,\n",
    "  });\n",
    "  let getFile = await ai.files.get({ name: file.name ?? \"\" });\n",
    "  while (getFile.state === FileState.PROCESSING) {\n",
    "    getFile = await ai.files.get({ name: file.name ?? \"\" });\n",
    "    console.log(`current file status (${getFile.displayName}): ${getFile.state ?? \"unknown\"}`);\n",
    "    console.log(\"File is still processing, retrying in 5 seconds\");\n",
    "\n",
    "    await new Promise((resolve) => {\n",
    "      setTimeout(resolve, 5000);\n",
    "    });\n",
    "  }\n",
    "  if (file.state === FileState.FAILED) {\n",
    "    throw new Error(\"File processing failed.\");\n",
    "  }\n",
    "  return file;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const textFile = await deferredFileUpload(transcriptFilePath, {\n",
    "  displayName: \"Apollo 11 Transcript\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Content\n",
    "\n",
    "After the file has been uploaded, you can make `client.models.generateContent` requests that reference the File API URI. Then you will ask the model to find a few lighthearted moments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are four lighthearted moments from the text:\n",
       "\n",
       "1.  **A Lost Bet Over Coffee:**\n",
       "    *   `00 00 54 13 CMP And tell Glenn Parker down at the Cape that he lucked out.`\n",
       "    *   `00 00 54 17 CC Understand. Tell Glenn Parker he lucked out.`\n",
       "    *   `00 00 54 22 CMP Yes. He lucked out. He doesn't owe me a cup of coffee.`\n",
       "    This exchange reveals a personal bet between Michael Collins and Glenn Parker, adding a touch of everyday human interaction to the high-stakes mission.\n",
       "\n",
       "2.  **Crew Distracted by the View:**\n",
       "    *   `01 03 15 30 CDR Yes, and he is eyeballing the Earth.`\n",
       "    *   `01 03 15 32 CMP He's got his head out the window.`\n",
       "    *   `01 03 15 35 CC I understand, I had trouble on 12 with him, too.`\n",
       "    The crew's playful comments about a fellow astronaut \"eyeballing the Earth\" and the CAPCOMM's relatable admission of having \"trouble with him, too\" highlight the human element of being captivated by the view from space.\n",
       "\n",
       "3.  **A Navy Term for Grayness:**\n",
       "    *   `01 03 22 57 LMP Yes. Is there a Navy term for that?`\n",
       "    *   `01 03 23 00 CC (Laughing.) A lot of gray paint.`\n",
       "    Buzz Aldrin asks a humorous, casual question about a \"Navy term\" for a visual phenomenon, and the CAPCOMM responds with laughter and a witty, simple answer, breaking the technical jargon.\n",
       "\n",
       "4.  **Zero-G Exercise and a TV Request:**\n",
       "    *   `01 06 51 33 CMP Ever alert and ... Hey, you got any medics down there watching high grade? I'm trying to do some running in place down here, and I'm wondering just out of curiosity whether it brings my heart rate up.`\n",
       "    *   `01 06 52 26 CC I'd like to see that sight. Why don't you give us a TV picture of that one.`\n",
       "    A crew member playfully asks about their heart rate while \"running in place\" in zero-g, leading the CAPCOMM to humorously request a TV broadcast of the unusual sight."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const response = await ai.models.generateContent({\n",
    "  model: MODEL_ID,\n",
    "  contents: [\n",
    "    \"Find four lighthearted moments in this text file.\",\n",
    "    google.createPartFromUri(textFile.uri ?? \"\", textFile.mimeType ?? \"text/plain\"),\n",
    "  ],\n",
    "});\n",
    "tslab.display.markdown(response.text ?? \"No response text available\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete File\n",
    "\n",
    "Files are automatically deleted after 2 days or you can manually delete them using `files.delete()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeleteFileResponse {}\n"
     ]
    }
   ],
   "source": [
    "await ai.files.delete({\n",
    "  name: textFile.name ?? \"\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning more\n",
    "\n",
    "The File API accepts files under 2GB in size and can store up to 20GB of files per project. Learn more about the [File API](../quickstarts/File_API.ipynb) here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
